{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_classifier_data():\n",
    "    data=pd.read_csv(\"WineQT.csv\")\n",
    "\n",
    "    values=[[] for i in data]\n",
    "    keys=[key for key in data]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        for key in range(len(keys)):\n",
    "            values[key].append(data[keys[key]][i])\n",
    "\n",
    "    X,Y=[],[]\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        xvals=[]\n",
    "        for key in keys:\n",
    "            if key == \"quality\":\n",
    "                if data[key][i]<=5:\n",
    "                    yvals=[ 0 for i in range(6)]\n",
    "                else:\n",
    "                    yvals=[ 1 for i in range(6)]\n",
    "                    \n",
    "            else:\n",
    "                xvals.append(data[key][i])\n",
    "        X.append(xvals)\n",
    "        Y.append(yvals)\n",
    "    # Y=np.array(Y)\n",
    "    # print(y)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "    scaler = MinMaxScaler()\n",
    "    X= scaler.fit_transform(X)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    x_train,x_oth,y_train,y_oth=train_test_split(X,Y,test_size=0.3,random_state=42)\n",
    "    x_val,x_test,y_val,y_test=train_test_split(x_oth,y_oth,test_size=0.1,random_state=42)\n",
    "    y_train=np.array(y_train)\n",
    "    return x_train,y_train,x_test,y_test,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "def get_regressor_data():\n",
    "    data=pd.read_csv(\"HousingData.csv\")\n",
    "\n",
    "    keys=[key for key in data]\n",
    "    column_means = data.mean()\n",
    "\n",
    "    data = data.fillna(column_means)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data = imputer.fit_transform(data)\n",
    "    values=[[] for i in range(len(data[0]))]\n",
    "    # print(keys)\n",
    "\n",
    "    for i in data:\n",
    "        for j in range(len(i)):\n",
    "            values[j].append(i[j])\n",
    "\n",
    "    X=data[:,:-1]\n",
    "    Y=data[:,-1]\n",
    "    Y=Y.reshape(-1,1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X= scaler.fit_transform(X)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    x_train,x_oth,y_train,y_oth=train_test_split(X,Y,test_size=0.3,random_state=42)\n",
    "    x_val,x_test,y_val,y_test=train_test_split(x_oth,y_oth,test_size=0.1,random_state=42)\n",
    "\n",
    "    y_train=y_train.reshape(-1,1)\n",
    "    y_test=y_test.reshape(-1,1)\n",
    "    y_val=y_val.reshape(-1,1)\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,x_val,y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "class RandomForestRegressor:\n",
    "    def __init__(self, n_estimators=100, max_features='sqrt', bootstrap=True, fraction_samples=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.fraction_samples = fraction_samples\n",
    "        self.estimators = []\n",
    "        self.feature_indices = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.max_features=n_features\n",
    "        n_subsamples = int(n_samples * self.fraction_samples)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                sample_indices = np.random.choice(n_samples, n_subsamples, replace=True)\n",
    "            else:\n",
    "                sample_indices = np.random.choice(n_samples, n_subsamples, replace=False)\n",
    "\n",
    "            feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "            self.feature_indices.append(feature_indices)\n",
    "            X_subset = X[sample_indices][:, feature_indices]\n",
    "            y_subset = y[sample_indices]\n",
    "            estimator = DecisionTreeRegressor()\n",
    "            estimator.fit(X_subset, y_subset)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i, estimator in enumerate(self.estimators):\n",
    "            feature_indices = self.feature_indices[i]\n",
    "            X_subset = X[:, feature_indices]\n",
    "            preds = estimator.predict(X_subset)\n",
    "            predictions.append(preds)\n",
    "\n",
    "        final_predictions = np.mean(np.array(predictions),axis=0)\n",
    "        return final_predictions\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators, bootstrap, fraction_samples):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bootstrap = bootstrap\n",
    "        self.fraction_samples = fraction_samples\n",
    "        self.estimators = []\n",
    "        self.feature_indices = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.max_features=n_features\n",
    "        n_subsamples = int(n_samples * self.fraction_samples)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            if self.bootstrap:\n",
    "                sample_indices = np.random.choice(n_samples, n_subsamples, replace=True)\n",
    "            else:\n",
    "                sample_indices = np.random.choice(n_samples, n_subsamples, replace=False)\n",
    "\n",
    "            feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "            self.feature_indices.append(feature_indices)\n",
    "            X_subset = X[sample_indices][:, feature_indices]\n",
    "            y_subset = y[sample_indices]\n",
    "            estimator = DecisionTreeClassifier()\n",
    "            estimator.fit(X_subset, y_subset)\n",
    "            self.estimators.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for i, estimator in enumerate(self.estimators):\n",
    "            feature_indices = self.feature_indices[i]\n",
    "            X_subset = X[:, feature_indices]\n",
    "            preds = estimator.predict(X_subset)\n",
    "            predictions.append(preds)\n",
    "        final_predictions = (np.mean(np.array(predictions),axis=0)>=0.5).astype(int)\n",
    "        return final_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571 \t Number of Estimators: 50 \t Bootstrap: True \t Fraction: 0.5\n",
      "Accuracy: 0.8285714285714286 \t Number of Estimators: 200 \t Bootstrap: True \t Fraction: 0.5\n",
      "Accuracy: 0.8285714285714286 \t Number of Estimators: 200 \t Bootstrap: False \t Fraction: 0.5\n",
      "Accuracy: 0.8285714285714286 \t Number of Estimators: 200 \t Bootstrap: False \t Fraction: 0.25\n",
      "Accuracy: 0.8285714285714286 \t Number of Estimators: 50 \t Bootstrap: True \t Fraction: 0.25\n",
      "Accuracy: 0.8285714285714286 \t Number of Estimators: 50 \t Bootstrap: False \t Fraction: 0.25\n",
      "Accuracy: 0.8 \t Number of Estimators: 100 \t Bootstrap: True \t Fraction: 1.0\n",
      "Accuracy: 0.8 \t Number of Estimators: 200 \t Bootstrap: True \t Fraction: 0.75\n",
      "Accuracy: 0.8 \t Number of Estimators: 100 \t Bootstrap: False \t Fraction: 0.75\n",
      "Accuracy: 0.8 \t Number of Estimators: 50 \t Bootstrap: True \t Fraction: 0.75\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,x_val,y_val = get_classifier_data()\n",
    "\n",
    "results = []\n",
    "\n",
    "for fraction in [0.15, 0.25, 0.5, 0.75, 1.0]:\n",
    "    for num_estimators in [10, 50, 100, 200]:\n",
    "        for bootstrap in [True, False]:\n",
    "            rf_classifier = RandomForestClassifier(fraction_samples=fraction, bootstrap=bootstrap, n_estimators=num_estimators)\n",
    "            rf_classifier.fit(x_train, y_train)\n",
    "            preds=rf_classifier.predict(x_test)\n",
    "            # print(len(preds))\n",
    "            # print(len(y_test))\n",
    "            score = accuracy_score(y_test, preds)\n",
    "            results.append([score, fraction, num_estimators, bootstrap])\n",
    "\n",
    "results.sort(reverse=True)\n",
    "# print(results[:10])\n",
    "for i in results[:10]:\n",
    "    print(\"Accuracy:\",i[0],\"\\t Number of Estimators:\", i[2],\"\\t Bootstrap:\",i[3],\"\\t Fraction:\", i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 8.758999999999991 \t Number of Estimators: 10 \t Bootstrap: True \t Fraction: 1.0\n",
      "Mean Squared Error: 9.075852500000003 \t Number of Estimators: 50 \t Bootstrap: False \t Fraction: 0.75\n",
      "Mean Squared Error: 11.166206249999995 \t Number of Estimators: 10 \t Bootstrap: False \t Fraction: 0.75\n",
      "Mean Squared Error: 11.297455999999979 \t Number of Estimators: 100 \t Bootstrap: False \t Fraction: 0.75\n",
      "Mean Squared Error: 11.997474999999994 \t Number of Estimators: 10 \t Bootstrap: False \t Fraction: 0.5\n",
      "Mean Squared Error: 14.058440218749965 \t Number of Estimators: 200 \t Bootstrap: False \t Fraction: 0.75\n",
      "Mean Squared Error: 15.364267515625013 \t Number of Estimators: 200 \t Bootstrap: True \t Fraction: 1.0\n",
      "Mean Squared Error: 15.849339249999998 \t Number of Estimators: 50 \t Bootstrap: False \t Fraction: 0.5\n",
      "Mean Squared Error: 16.93525731249996 \t Number of Estimators: 100 \t Bootstrap: True \t Fraction: 1.0\n",
      "Mean Squared Error: 17.394356499999972 \t Number of Estimators: 50 \t Bootstrap: True \t Fraction: 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,x_val,y_val = get_regressor_data()\n",
    "\n",
    "results= []\n",
    "\n",
    "for fraction in [0.15, 0.25, 0.5, 0.75, 1.0]:\n",
    "    for num_estimators in [10, 50, 100, 200]:\n",
    "        for bootstrap in [True, False]:\n",
    "            rf_regressor = RandomForestRegressor(fraction_samples=fraction, bootstrap=bootstrap, n_estimators=num_estimators)\n",
    "            rf_regressor.fit(x_train, y_train)\n",
    "            preds=rf_regressor.predict(x_test)\n",
    "            score = mean_squared_error(y_test, preds)\n",
    "            results.append([score, fraction, num_estimators, bootstrap])\n",
    "\n",
    "results.sort()\n",
    "for i in results[:10]:\n",
    "    print(\"Mean Squared Error:\",i[0],\"\\t Number of Estimators:\", i[2],\"\\t Bootstrap:\",i[3],\"\\t Fraction:\", i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alphas = []\n",
    "        self.classifier = []\n",
    "        self.num_of_iter = None\n",
    "        self.training_errors = []\n",
    "        self.prediction_errors = []\n",
    "        \n",
    "    def fit(self, X, y, num_of_iter):\n",
    "        self.alphas = [] \n",
    "        self.training_errors = []\n",
    "        self.num_of_iter = num_of_iter\n",
    "        for m in range(0, num_of_iter):\n",
    "            \n",
    "            if m == 0:\n",
    "                weights_i = np.ones(len(y)) * 1 / len(y)\n",
    "            else:\n",
    "                weights_i = self.update_weights(weights_i, alpha_m, y, y_pred)\n",
    "            \n",
    "            classifier = DecisionTreeClassifier(max_depth = 10)\n",
    "            classifier.fit(X, y, sample_weight = weights_i)\n",
    "            self.classifier.append(classifier)\n",
    "            y_pred = classifier.predict(X)\n",
    "            \n",
    "            error_m = self.compute_error(y, y_pred, weights_i)\n",
    "            self.training_errors.append(error_m)\n",
    "\n",
    "            alpha_m = self.compute_alpha(error_m)\n",
    "            self.alphas.append(alpha_m)\n",
    "\n",
    "        # assert len(self.classifier) == len(self.alphas)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.num_of_iter)) \n",
    "\n",
    "        for m in range(self.num_of_iter):\n",
    "            y_pred_m = self.classifier[m].predict(X) * self.alphas[m]\n",
    "            weak_preds.iloc[:,m] = y_pred_m\n",
    "\n",
    "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def compute_error(self, y, y_pred, weights_i):\n",
    "        return (sum(weights_i * (np.not_equal(y, y_pred)).astype(int)))/sum(weights_i)\n",
    "\n",
    "    def compute_alpha(self, error):\n",
    "        return np.log((1 - error) / error)\n",
    "\n",
    "    def update_weights(self, weights_i, alpha, y, y_pred):\n",
    "        return weights_i * np.exp(alpha * (np.not_equal(y, y_pred)).astype(int))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train,y_train,x_test,y_test,x_val,y_val = get_classifier_data()\n",
    "y_train=y_train.tolist()\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i]=y_test[i][0]\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i]=y_train[i][0]\n",
    "for i in range(len(y_val)):\n",
    "    y_val[i]=y_val[i][0]\n",
    "\n",
    "training_times = []\n",
    "accuracy_values = []\n",
    "\n",
    "num_estimators_list = [10, 50, 100, 200, 500]\n",
    "for m in num_estimators_list:\n",
    "    start_time = time.time()\n",
    "    classifier = AdaBoostClassifier()\n",
    "    classifier.fit(x_train, y_train, M = m)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    accuracy_values.append(score)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(num_estimators_list, training_times, marker='o')\n",
    "plt.title('Training Time vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(num_estimators_list, accuracy_values, marker='o', color='r')\n",
    "plt.title('Accuracy vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaBoostRegressor:\n",
    "    \n",
    "    def fit(self, X_train, y_train, total_iter, depth, random_state = None):\n",
    "        \n",
    "        self.y_train = y_train\n",
    "        self.X_train = X_train\n",
    "        self.total_iter = total_iter\n",
    "        self.depth = depth\n",
    "        self.N, self.D = X_train.shape\n",
    "        self.weights = np.repeat(1/self.N, self.N)\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.trees = []    \n",
    "        self.weak_weights = []\n",
    "        self.fitted_values = np.empty((self.N, self.total_iter))\n",
    "        for t in range(self.total_iter):\n",
    "            \n",
    "            bootstrap_indices = np.random.choice(np.arange(self.N), size = self.N, replace = True, p = self.weights)\n",
    "            bootstrap_X = self.X_train[bootstrap_indices]\n",
    "            bootstrap_y = self.y_train[bootstrap_indices]   \n",
    "            \n",
    "            tree = DecisionTreeRegressor(max_depth = depth)\n",
    "            tree.fit(bootstrap_X, bootstrap_y)\n",
    "            y_ = tree.predict(X_train)\n",
    "\n",
    "            self.trees.append(tree)\n",
    "            self.fitted_values[:,t] = y_\n",
    "            \n",
    "            abs_errors_t = np.abs(self.y_train - y_)\n",
    "            max_abs_err = np.max(abs_errors_t)\n",
    "            norm_errors = abs_errors_t/max_abs_err\n",
    "            weighted_norm_err = np.sum(self.weights*norm_errors)\n",
    "\n",
    "            if weighted_norm_err >= 0.5:\n",
    "                self.T = t - 1\n",
    "                self.fitted_values = self.fitted_values[:,:t-1]\n",
    "                self.trees = self.trees[:t-1]\n",
    "                continue\n",
    "            \n",
    "            weak_learner_weight = weighted_norm_err/(1 - weighted_norm_err)\n",
    "            self.weak_weights.append(weak_learner_weight)\n",
    "\n",
    "            norm_factor = np.sum(self.weights*weak_learner_weight**(1-norm_errors))\n",
    "\n",
    "            self.weights *= weak_learner_weight**(1-norm_errors)/norm_factor\n",
    "            \n",
    "        self.model_weights = np.log(1/np.array(self.weak_weights))\n",
    "        self.y_train_hat = np.array([self.weighted_median(self.fitted_values[n], self.model_weights) for n in range(self.N)])\n",
    "\n",
    "    def weighted_median(self, values, weights):    \n",
    "        sorted_indices = values.argsort()\n",
    "        values = values[sorted_indices]\n",
    "        weights = weights[sorted_indices]\n",
    "        weights_cumulative_sum = weights.cumsum()\n",
    "        median_weight = np.argmax(weights_cumulative_sum >= sum(weights)/2)\n",
    "        return values[median_weight]\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        N_test = len(X_test)\n",
    "        fitted_values = np.empty((N_test, self.T))\n",
    "        for t, tree in enumerate(self.trees):\n",
    "            fitted_values[:,t] = tree.predict(X_test)\n",
    "        return np.array([self.weighted_median(fitted_values[n], self.model_weights) for n in range(N_test)]) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x_train,y_train,x_test,y_test,x_val,y_val = get_regressor_data()\n",
    "x_train,y_train,x_test,y_test,x_val,y_val=np.array(x_train),np.array(y_train),np.array(x_test),np.array(y_test),np.array(x_val),np.array(y_val)\n",
    "\n",
    "training_times=[]\n",
    "accuracy_values=[]\n",
    "num_estimators_list = [10, 50, 100, 200, 500]\n",
    "for m in num_estimators_list:\n",
    "    start_time = time.time()\n",
    "    booster = AdaBoostRegressor()\n",
    "    y_train=y_train.reshape(-1)\n",
    "    booster.fit(x_train, y_train, T = m, stub_depth = 4, random_state = 123)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "\n",
    "    score=mean_squared_error(y_test, booster.predict(x_test))\n",
    "    accuracy_values.append(score)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(num_estimators_list, training_times, marker='o')\n",
    "plt.title('Training Time vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(num_estimators_list, accuracy_values, marker='o', color='r')\n",
    "plt.title('Mean Squared Error (MSE) vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Squared Error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        class_prob = np.bincount(y) / len(y)\n",
    "        self.initial_prediction = np.log(class_prob[1] / class_prob[0])\n",
    "\n",
    "        prediction = np.full(y.shape, self.initial_prediction)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            class_prob = 1 / (1 + np.exp(-prediction))\n",
    "            residuals = y - class_prob\n",
    "\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            threshold = 0.5\n",
    "            class_labels = np.where(residuals > threshold, 1, 0)\n",
    "            tree.fit(X, class_labels)\n",
    "            tree_pred = tree.predict(X)\n",
    "            prediction = prediction + self.learning_rate * tree_pred\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.full(X.shape[0], self.initial_prediction)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            tree_pred = tree.predict(X)\n",
    "            prediction += self.learning_rate * tree_pred\n",
    "        \n",
    "        prob = 1 / (1 + np.exp(-prediction))\n",
    "        return np.where(prob >= 0.5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test,x_val,y_val = get_classifier_data()\n",
    "\n",
    "# print(y_train)\n",
    "y_train=y_train.tolist()\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i]=y_test[i][0]\n",
    "for i in range(len(y_train)):\n",
    "    y_train[i]=y_train[i][0]\n",
    "for i in range(len(y_val)):\n",
    "    y_val[i]=y_val[i][0]\n",
    "y_train=np.array(y_train)\n",
    "\n",
    "training_times=[]\n",
    "num_estimators_list = [10, 50, 100, 200, 500]\n",
    "for num_estimators in num_estimators_list:\n",
    "    start_time = time.time()\n",
    "    gb_classifier = GradientBoostingClassifier(n_estimators=num_estimators, learning_rate=0.1)\n",
    "    gb_classifier.fit(x_train, y_train)\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "\n",
    "    predictions = gb_classifier.predict(x_test)\n",
    "    score=accuracy_score(y_test, predictions)\n",
    "    accuracy_values.append(score)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(num_estimators_list, training_times, marker='o')\n",
    "plt.title('Training Time vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(num_estimators_list, accuracy_values, marker='o', color='r')\n",
    "plt.title('Accuracy vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        prediction = np.full(y.shape, self.initial_prediction)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - prediction\n",
    "\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            tree_pred = tree.predict(X)\n",
    "\n",
    "            tree_pred = np.reshape(tree_pred, prediction.shape)\n",
    "\n",
    "            prediction = prediction + self.learning_rate * tree_pred\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prediction = np.full(X.shape[0], self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            tree_pred = tree.predict(X)\n",
    "            prediction += self.learning_rate * tree_pred\n",
    "        \n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train, y_train, x_test, y_test, x_val, y_val = get_regressor_data()\n",
    "\n",
    "training_times = []\n",
    "mse_values = []\n",
    "\n",
    "num_estimators_list = [10, 50, 100, 200, 500]\n",
    "for num_estimators in num_estimators_list:\n",
    "    start_time = time.time()\n",
    "    gb_regressor = GradientBoostingRegressor(n_estimators=num_estimators, learning_rate=0.1, max_depth=3)\n",
    "    \n",
    "    gb_regressor.fit(x_train, y_train)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "\n",
    "    predictions = gb_regressor.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mse_values.append(mse)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_estimators_list, training_times, marker='o')\n",
    "plt.title('Training Time vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_estimators_list, mse_values, marker='o', color='r')\n",
    "plt.title('Mean Squared Error (MSE) vs Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Squared Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
